{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f40c232-df6e-49df-9016-6459e4af2e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Coding Attention in PyTorch!!!\n",
    "\n",
    "By Josh Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ad8fb-fc53-4f24-bca5-84ad6814d993",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4226d63-8d76-40bc-a8e6-0f290a159418",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "In this tutorial, we will code a class that is capable of all **3** types of **Attention** that we have studied, **Self-Attention**, **Masked Self-Attention**, and **Encoder-Decoder Attention**. We'll also code a few lines that will make **Multi-Headed Attention** work.\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Code an Attention Class!!!](#attention)** This class will be able to perform **Self-Attention**, **Masked-Self Attention**, and **Encoder-Decoder Attention**.\n",
    "\n",
    "- **[Calculate Encoder-Decoder Attention Values!!!](#calculate)** We'll then use the class that we created, Attention, to calculate **Encoder-Decoder Attention** values for some sample data.\n",
    " \n",
    "- **[Code Multi-Head Attention!!!](#multi)** We'll code **Multi-Head Attention**.\n",
    "\n",
    "- **[Calculate Mult-Head Attention!!!!](#calcMulti)** Lastly, we calculate **Multi-Head Attention** values for some sample data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d3a52-a43e-44cf-b8b6-b2ce88bea382",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b86036-1369-441c-a14d-3ba7bdaa103b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import the modules that will do all the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c520e0b-c6e4-43ce-93f5-c0f2b5e75438",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f76ab-09d4-4573-924c-75351d7955e8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58ebb2-1798-41c3-9ff8-1b4f50605964",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c4f0d-83b6-488f-8d91-66c54776bf2f",
   "metadata": {},
   "source": [
    "# Code Attention\n",
    "<a id=\"attention\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af7679",
   "metadata": {},
   "source": [
    "## Again Masked self-attention for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "979d3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "        \n",
    "    def forward(self, token_encodings, mask=None):\n",
    "\n",
    "        q = self.W_q(token_encodings)\n",
    "        k = self.W_k(token_encodings)\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            ## Here we are masking out things we don't want to pay attention to\n",
    "            ##\n",
    "            ## We replace values we wanted masked out\n",
    "            ## with a very small negative number so that the SoftMax() function\n",
    "            ## will give all masked elements an output value (or \"probability\") of 0.\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9) # I've also seen -1e20 and -9e15 used in masking\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba652ae",
   "metadata": {},
   "source": [
    "## Attention implemneted for this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3392130-cd25-4000-97bb-9612764c83a8",
   "metadata": {
    "height": 625
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "\n",
    "    ## The only change from SelfAttention and attention is that\n",
    "    ## now we expect 3 sets of encodings to be passed in...\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            \n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2c013",
   "metadata": {},
   "source": [
    "## My Previous implementation of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c11295cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1600,  0.2300],\n",
      "        [ 0.5700,  1.3600],\n",
      "        [ 4.4100, -2.1600]])\n",
      "mask: tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "2\n",
      "torch.Size([3, 2])\n",
      "My attention: tensor([[ 0.6038,  0.7434],\n",
      "        [-0.0062,  0.6072],\n",
      "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)\n",
      "Attention: tensor([[ 0.6038,  0.7434],\n",
      "        [-0.0062,  0.6072],\n",
      "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyAttention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, \n",
    "                 emb_d=2,\n",
    "                 token_emb_dim_idx=0,\n",
    "                 emb_dim_idx=1):\n",
    "        \"\"\"\n",
    "        Basically upgrades the entering token encodings by:\n",
    "        1. Calculating similarity scores with the other tokens by the information learned in\n",
    "        two matrices: q and k\n",
    "        2. Scale those scores by sqrt(emb_dim_of_k)\n",
    "        3. Calculate weights out of those scores with softmax\n",
    "        4. Calculate the influence of those scores on the values though the info learn in v.\n",
    "        \n",
    "        emb_d : the number of embedding values per token.\n",
    "        token_emb_dim_idx: the index of the dimension signaling each token (rows)\n",
    "        emb_dim_idx: the index of the dimension of the embedding size (columns)\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_d = emb_d\n",
    "        self.token_emb_dim_idx=token_emb_dim_idx\n",
    "        self.emb_dim_idx=emb_dim_idx\n",
    "        self.q_w = nn.Linear(in_features=emb_d, out_features=emb_d, bias=False)\n",
    "        self.k_w = nn.Linear(in_features=emb_d, out_features=emb_d, bias=False)\n",
    "        self.v_w = nn.Linear(in_features=emb_d, out_features=emb_d, bias=False)\n",
    "                \n",
    "    def forward(self, token_encodings: torch.Tensor, mask: torch.Tensor = None):\n",
    "        q = self.q_w(token_encodings)\n",
    "        print(q.shape)  # Note the shape is 3x2 now, as token_encodings is a 3x2 matrix which is \n",
    "        # multiplied by q, which is a 2x2 matrix\n",
    "        k = self.k_w(token_encodings)\n",
    "        v = self.v_w(token_encodings)\n",
    "        \n",
    "        similarity_scores = torch.matmul(q, torch.transpose(k, dim0=self.token_emb_dim_idx, dim1=self.emb_dim_idx))\n",
    "        scaled_scores = similarity_scores / torch.tensor(self.emb_d ** 0.5)\n",
    "        if mask is not None:\n",
    "            scaled_scores.masked_fill_(mask, value=-9e15)\n",
    "        softmax_scores = F.softmax(scaled_scores, dim=self.emb_dim_idx)\n",
    "        weighted_scores = torch.matmul(softmax_scores, v)\n",
    "        return weighted_scores\n",
    "    \n",
    "## create a matrix of token encodings (which are word embedings + positional encoding)...\n",
    "## The first dimension of the encoding matrix refers to the token encodings themselves\n",
    "## and the second dimension refers to the embedding dimensions of each token\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "print(encodings_matrix)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "mask = torch.ones((3,3))\n",
    "mask = torch.tril(mask)\n",
    "mask = mask == 0\n",
    "\n",
    "print(f\"mask: {mask}\")\n",
    "print(encodings_matrix.shape[1])\n",
    "my_attention = MyAttention(encodings_matrix.shape[1])\n",
    "print(f\"My attention: {my_attention(encodings_matrix, mask)}\")\n",
    "torch.manual_seed(42)\n",
    "attention = MaskedSelfAttention(encodings_matrix.shape[1])\n",
    "print(f\"This Attention: {attention(encodings_matrix, mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62970aef-efbe-400e-b35f-bb5aeb0eafe7",
   "metadata": {},
   "source": [
    "# BAM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371cd1e0-0216-438d-a241-05533e4b374d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75aaaa3-29e0-4c03-8a11-816bd4b7aea3",
   "metadata": {},
   "source": [
    "# Calculate Encoder-Decoder Attention\n",
    "<a id=\"calculate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0946c6d7-9960-4f49-8aa6-20dd614efbdc",
   "metadata": {
    "height": 404
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create matrices of token encodings...\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "attention = Attention(d_model=2,\n",
    "                      row_dim=0,\n",
    "                      col_dim=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "attention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1153eec7-5573-41f7-b7de-3106b2ca07f9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7c8c7-1d02-4064-8302-7bcef7c67dc4",
   "metadata": {},
   "source": [
    "# Code Mutli-Head Attention\n",
    "<a id=\"multi\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36d5a6a6-3348-40c3-ac6a-b90a47b400d3",
   "metadata": {
    "height": 489
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1, \n",
    "                 num_heads=1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        ## create a bunch of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_model, row_dim, col_dim) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        self.col_dim = col_dim\n",
    "        \n",
    "    def forward(self, \n",
    "                encodings_for_q, \n",
    "                encodings_for_k,\n",
    "                encodings_for_v):\n",
    "\n",
    "        ## run the data through all of the attention heads\n",
    "        return torch.cat([head(encodings_for_q, \n",
    "                               encodings_for_k,\n",
    "                               encodings_for_v) \n",
    "                          for head in self.heads], dim=self.col_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fde1b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "torch.Size([3, 3])\n",
      "Q shape as input: torch.Size([3, 4])\n",
      "Q shape after reshaping: torch.Size([3, 2, 2])\n",
      "Q shape after transponsing: torch.Size([2, 3, 2])\n",
      "scaled scores before (torch.Size([2, 3, 3]))\n",
      "softmax scores (torch.Size([2, 3, 3])):\n",
      "\n",
      "My multihead attention with no mask: tensor([[-0.1703,  0.7427],\n",
      "        [-0.0230,  1.0481],\n",
      "        [-0.2958,  0.2219]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyMultiHeadAttention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, \n",
    "                 emb_dim=2,\n",
    "                 token_emb_dim_idx=0,\n",
    "                 emb_dim_idx=1, \n",
    "                 num_heads=1):\n",
    "        \"\"\"\n",
    "        Basically upgrades the entering token encodings by:\n",
    "        1. Calculating similarity scores with the other tokens by the information learned in\n",
    "        two matrices: q and k\n",
    "        2. Scale those scores by sqrt(emb_dim_of_k)\n",
    "        3. Calculate weights out of those scores with softmax\n",
    "        4. Calculate the influence of those scores on the values though the info learn in v.\n",
    "        \n",
    "        emb_dim : the number of embedding values per token.\n",
    "        token_emb_dim_idx: the index of the dimension signaling each token (rows)\n",
    "        emb_dim_idx: the index of the dimension of the embedding size (columns)\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim\n",
    "        self.q_w = nn.Linear(in_features=emb_dim, out_features=emb_dim * num_heads, bias=False)\n",
    "        self.k_w = nn.Linear(in_features=emb_dim, out_features=emb_dim * num_heads, bias=False)\n",
    "        self.v_w = nn.Linear(in_features=emb_dim, out_features=emb_dim * num_heads, bias=False)\n",
    "        self.out_proj = nn.Linear(in_features=emb_dim * num_heads, out_features=emb_dim, bias=False)\n",
    "                \n",
    "    def forward(self, token_encodings: torch.Tensor, mask: torch.Tensor = None):        \n",
    "        seq_len, _ = token_encodings.shape  # Expecting (T, D)\n",
    "        q = self.q_w(token_encodings)\n",
    "        # multiplied by q, which is a 2x2 matrix\n",
    "        k = self.k_w(token_encodings)\n",
    "        v = self.v_w(token_encodings)\n",
    "        print(f\"Q shape as input: {q.shape}\")  # Note the shape is 3x2 now, as token_encodings is a 3x2 matrix which is \n",
    "        \n",
    "         # Reshape into multiple heads: (T, emb_dim) ‚Üí (T, num_heads, head_dim) ‚Üí (num_heads, T, head_dim), so (3,2) to (3,1,2) to (1, 3, 2)\n",
    "        q = q.view(seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(seq_len, self.num_heads, self.head_dim)\n",
    "        print(f\"Q shape after reshaping: {q.shape}\")  # Has to be (1, 3, 2)\n",
    "        \n",
    "        q = q.transpose(0, 1)\n",
    "        k = k.transpose(0, 1)\n",
    "        v = v.transpose(0, 1)\n",
    "        print(f\"Q shape after transponsing: {q.shape}\")  # Has to be (1, 3, 2) (num_heads, T, head_dim)\n",
    "        \n",
    "        similarity_scores = torch.matmul(q, k.transpose(dim0=-2, dim1=-1))\n",
    "        scaled_scores = similarity_scores / (self.head_dim ** 0.5)\n",
    "        print(f\"scaled scores before ({scaled_scores.shape})\") #:\\n{scaled_scores}\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_scores = scaled_scores.masked_fill(mask.unsqueeze(0) == False, value=-1e9)\n",
    "            print(f\"scaled scores after ({scaled_scores.shape}):\\n{scaled_scores}\")\n",
    "        \n",
    "        softmax_scores = F.softmax(scaled_scores, dim=-1)\n",
    "        print(f\"softmax scores ({scaled_scores.shape}):\\n\") #v scores: {v.shape}\")\n",
    "        \n",
    "        weighted_scores = torch.matmul(softmax_scores, v)\n",
    "        \n",
    "        # Reshape back: (T, num_heads, head_dim) ‚Üí (T, emb_dim)\n",
    "        weighted_scores = weighted_scores.transpose(0,1).reshape(seq_len, self.num_heads * self.head_dim)\n",
    "        \n",
    "        # Final projection back to original embedding dimension\n",
    "        output = self.out_proj(weighted_scores)  # (T, D)\n",
    "        return output\n",
    "    \n",
    "## create a matrix of token encodings (which are word embedings + positional encoding)...\n",
    "## The first dimension of the encoding matrix refers to the token encodings themselves\n",
    "## and the second dimension refers to the embedding dimensions of each token\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N_HEADS = 2\n",
    "mask = torch.ones((3,3))\n",
    "mask = torch.tril(mask)\n",
    "mask = mask == 0\n",
    "print(f\"mask: {mask}\\n{mask.shape}\")\n",
    "\n",
    "my_attention = MyMultiHeadAttention(encodings_matrix.shape[1], num_heads=N_HEADS)\n",
    "print(f\"My multihead attention with no mask: {my_attention(encodings_matrix)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "m_attention = MaskedSelfAttention(encodings_matrix.shape[1])\n",
    "print(f\"Masked Attention with no mask: {m_attention(encodings_matrix)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "556c6e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: tensor([[1.0100, 1.0641],\n",
      "        [0.2040, 0.7057],\n",
      "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)\n",
      "Tihs MultiHeadAttention: tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
      "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
      "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "regular_attention = Attention(encodings_matrix.shape[1])\n",
    "print(f\"Attention: {regular_attention(encodings_for_q, encodings_for_k, encodings_for_v)}\")\n",
    "torch.manual_seed(42)\n",
    "mh_attention = MultiHeadAttention(encodings_matrix.shape[1], num_heads=N_HEADS)\n",
    "print(f\"Tihs MultiHeadAttention: {mh_attention(encodings_for_q, encodings_for_k, encodings_for_v)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a798f50-cf6e-4643-bf88-97adbe81c202",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241b0b7-ade3-45f5-985d-7b370d77ec76",
   "metadata": {},
   "source": [
    "# Calcualte Multi-Head Attention\n",
    "<a id=\"calcMulti\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607d714-bdcd-4d6a-a826-8c8c51802e72",
   "metadata": {},
   "source": [
    "First, verify that we can still correctly calculate attention with a single head..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b022438-acaa-4fff-8581-748c2151aebb",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea43a62-94ea-430b-a738-3ab12b4fc6ba",
   "metadata": {},
   "source": [
    "Second, calculate attention with multiple heads..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec152f85",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
       "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
       "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=2)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c4d95-8904-4a84-9c5f-6dcf522eb37d",
   "metadata": {},
   "source": [
    "# TRIPLE BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b640c1",
   "metadata": {},
   "source": [
    "# Another MH Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f675465",
   "metadata": {},
   "source": [
    "### Option 1. Split the embedding dimension (standard approach)\n",
    "\n",
    "Set things up so that the projections go from [T, emb_dim] to [T, emb_dim] (not to [T, emb_dim √ó num_heads]), then split that into num_heads pieces. This means each head gets a slice of size\n",
    "\n",
    "\\text{head\\_dim} = \\frac{\\text{emb\\_dim}}{\\text{num\\_heads}}\n",
    "\n",
    "so that the total remains emb_dim. In this case, if you set num_heads = 1 you get the full embedding per token, but if you set num_heads = 2 each head sees half the features. (That‚Äôs why head‚ÄØ0 from a 2‚Äêhead model won‚Äôt match a single-head model‚Äîthe projections are done on a smaller slice.) This is the most common approach in Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1d9cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-head output:\n",
      "tensor([[1.0100, 1.0641],\n",
      "        [0.2040, 0.7057],\n",
      "        [3.4989, 2.2427]], grad_fn=<ViewBackward0>)\n",
      "Weights:\n",
      "tensor([[[0.1403, 0.0845, 0.7752],\n",
      "         [0.0292, 0.0123, 0.9586],\n",
      "         [0.3715, 0.2413, 0.3872]],\n",
      "\n",
      "        [[0.3656, 0.3994, 0.2350],\n",
      "         [0.2863, 0.2599, 0.4538],\n",
      "         [0.3330, 0.6550, 0.0119]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Multi-head (2 heads) output:\n",
      "tensor([[0.8080, 1.2760],\n",
      "        [0.6116, 0.7280],\n",
      "        [0.6064, 2.4013]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Weights:\n",
      "tensor([[[0.1403, 0.0845, 0.7752],\n",
      "         [0.0292, 0.0123, 0.9586],\n",
      "         [0.3715, 0.2413, 0.3872]],\n",
      "\n",
      "        [[0.3656, 0.3994, 0.2350],\n",
      "         [0.2863, 0.2599, 0.4538],\n",
      "         [0.3330, 0.6550, 0.0119]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Multi-head (2 heads) output:\n",
      "tensor([[[0.8080],\n",
      "         [0.6116],\n",
      "         [0.6064]],\n",
      "\n",
      "        [[1.2760],\n",
      "         [0.7280],\n",
      "         [2.4013]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Weights:\n",
      "tensor([[[0.1403, 0.0845, 0.7752],\n",
      "         [0.0292, 0.0123, 0.9586],\n",
      "         [0.3715, 0.2413, 0.3872]],\n",
      "\n",
      "        [[0.3656, 0.3994, 0.2350],\n",
      "         [0.2863, 0.2599, 0.4538],\n",
      "         [0.3330, 0.6550, 0.0119]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim=2, token_emb_dim_idx=0, emb_dim_idx=1, num_heads=1):\n",
    "        super().__init__()\n",
    "        assert emb_dim % num_heads == 0, \"emb_dim must be divisible by num_heads\"\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "\n",
    "        # These layers now map from [T, emb_dim] to [T, emb_dim]\n",
    "        self.q_w = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False)\n",
    "        self.k_w = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False)\n",
    "        self.v_w = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=False)\n",
    "    \n",
    "    def forward(self, token_encoding, mask=None, concatenate_heads: bool = True):\n",
    "        T = token_encoding.size(0)  # assuming tokens are in dimension 0\n",
    "        # 1. Project to queries, keys, and values: shape [T, emb_dim]\n",
    "        q = self.q_w(token_encoding)\n",
    "        k = self.k_w(token_encoding)\n",
    "        v = self.v_w(token_encoding)\n",
    "        \n",
    "        # 2. Split into heads: reshape to [T, num_heads, head_dim]\n",
    "        q = q.view(T, self.num_heads, self.head_dim)\n",
    "        k = k.view(T, self.num_heads, self.head_dim)\n",
    "        v = v.view(T, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 3. Transpose to get [num_heads, T, head_dim]\n",
    "        q = q.transpose(0, 1)\n",
    "        k = k.transpose(0, 1)\n",
    "        v = v.transpose(0, 1)\n",
    "        \n",
    "        # 4. Scaled dot-product attention per head\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # shape: [num_heads, T, head_dim]\n",
    "        \n",
    "        if concatenate_heads:\n",
    "            # 5. Concatenate heads back: reshape to [T, emb_dim]\n",
    "            attn_output = attn_output.transpose(0, 1).reshape(T, self.emb_dim)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "torch.manual_seed(42)\n",
    "token_encoding = torch.tensor([[1.16, 0.23],\n",
    "                               [0.57, 1.36],\n",
    "                               [4.41, -2.16]])\n",
    "# With num_heads=1, head_dim = 2 ‚Üí same as single-head attention.\n",
    "mha_single = MyMultiHeadAttention(emb_dim=2, num_heads=1)\n",
    "out_single, attn_weigths = mha_single(token_encoding)\n",
    "print(f\"Single-head output:\\n{out_single}\\nWeights:\\n{attn_weights}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# With num_heads=2, head_dim = 1 ‚Üí each head gets half the features.\n",
    "mha_multi = MyMultiHeadAttention(emb_dim=2, num_heads=2)\n",
    "out_multi, attn_weigths = mha_multi(token_encoding)\n",
    "print(f\"\\nMulti-head (2 heads) output:\\n{out_multi}\\nWeights:\\n{attn_weights}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "out_multi, attn_weigths = mha_multi(token_encoding, concatenate_heads=False)\n",
    "print(f\"\\nMulti-head (2 heads) output:\\n{out_multi}\\nWeights:\\n{attn_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d6479",
   "metadata": {},
   "source": [
    "### Option 2. Preserve full embedding per head and use a final projection\n",
    "\n",
    "Goal:t one of the heads (say head‚ÄØ0) behaves exactly as the single-head attention does, then each head must process the full embedding. In that case, linear layers will map from [T, emb_dim] to [T, emb_dim * num_heads] (so that each head gets an emb_dim-dimensional representation), and then‚Äîafter computing attention independently per head‚Äîyou combine them with a final linear layer that projects from the concatenated space back to emb_dim. \n",
    "\n",
    "This means if you set num_heads = 1, you get the same computation as the single-head case, and for num_heads = 2 the first head‚Äôs result will match the single-head‚Äôs output (provided you initialize that head‚Äôs parameters identically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b988535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-head output:\n",
      "tensor([[1.0100, 1.0641],\n",
      "        [0.2040, 0.7057],\n",
      "        [3.4989, 2.2427]], grad_fn=<ViewBackward0>)\n",
      "Weights:\n",
      "tensor([[[0.3573, 0.4011, 0.2416],\n",
      "         [0.3410, 0.6047, 0.0542],\n",
      "         [0.0722, 0.0320, 0.8959]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Multi-head (2 heads) output:\n",
      "tensor([[[ 1.8188, -1.4734],\n",
      "         [ 2.1126, -1.7779],\n",
      "         [ 1.1966, -0.8276]],\n",
      "\n",
      "        [[-0.5599, -0.4288],\n",
      "         [-0.7620, -0.9759],\n",
      "         [-0.3427,  0.2084]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Weights:\n",
      "tensor([[[0.1403, 0.0845, 0.7752],\n",
      "         [0.0292, 0.0123, 0.9586],\n",
      "         [0.3715, 0.2413, 0.3872]],\n",
      "\n",
      "        [[0.3656, 0.3994, 0.2350],\n",
      "         [0.2863, 0.2599, 0.4538],\n",
      "         [0.3330, 0.6550, 0.0119]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Multi-head (2 heads) output:\n",
      "tensor([[ 1.8188, -1.4734, -0.5599, -0.4288],\n",
      "        [ 2.1126, -1.7779, -0.7620, -0.9759],\n",
      "        [ 1.1966, -0.8276, -0.3427,  0.2084]], grad_fn=<UnsafeViewBackward0>)\n",
      "Weights:\n",
      "tensor([[[0.1403, 0.0845, 0.7752],\n",
      "         [0.0292, 0.0123, 0.9586],\n",
      "         [0.3715, 0.2413, 0.3872]],\n",
      "\n",
      "        [[0.3656, 0.3994, 0.2350],\n",
      "         [0.2863, 0.2599, 0.4538],\n",
      "         [0.3330, 0.6550, 0.0119]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Multi-head (2 heads) output:\n",
      "tensor([[-0.1703,  0.7427],\n",
      "        [-0.0230,  1.0481],\n",
      "        [-0.2958,  0.2219]], grad_fn=<MmBackward0>)\n",
      "Weights:\n",
      "tensor([[[0.1403, 0.0845, 0.7752],\n",
      "         [0.0292, 0.0123, 0.9586],\n",
      "         [0.3715, 0.2413, 0.3872]],\n",
      "\n",
      "        [[0.3656, 0.3994, 0.2350],\n",
      "         [0.2863, 0.2599, 0.4538],\n",
      "         [0.3330, 0.6550, 0.0119]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim=2, token_emb_dim_idx=0, emb_dim_idx=1, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads        \n",
    "        # Here we keep each head's dimension equal to emb_dim.\n",
    "        self.head_dim = emb_dim  \n",
    "        # Project from [T, emb_dim] to [T, emb_dim * num_heads]\n",
    "        self.q_w = nn.Linear(in_features=emb_dim, out_features=emb_dim * num_heads, bias=False)\n",
    "        self.k_w = nn.Linear(in_features=emb_dim, out_features=emb_dim * num_heads, bias=False)\n",
    "        self.v_w = nn.Linear(in_features=emb_dim, out_features=emb_dim * num_heads, bias=False)\n",
    "        # Final projection to bring concatenated heads back to emb_dim.\n",
    "        self.out_proj = nn.Linear(in_features=emb_dim * num_heads, out_features=emb_dim, bias=False)\n",
    "    \n",
    "    def forward(self, token_encoding, mask=None, concatenate_heads: bool = True, project: bool = False):\n",
    "        T = token_encoding.size(0)\n",
    "        q = self.q_w(token_encoding)  # shape: [T, emb_dim*num_heads]\n",
    "        k = self.k_w(token_encoding)\n",
    "        v = self.v_w(token_encoding)\n",
    "        \n",
    "        # Reshape to separate heads: [T, num_heads, head_dim]\n",
    "        q = q.view(T, self.num_heads, self.head_dim)\n",
    "        k = k.view(T, self.num_heads, self.head_dim)\n",
    "        v = v.view(T, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose: [num_heads, T, head_dim]\n",
    "        q = q.transpose(0, 1)\n",
    "        k = k.transpose(0, 1)\n",
    "        v = v.transpose(0, 1)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # shape: [num_heads, T, head_dim]\n",
    "        \n",
    "        if concatenate_heads:            \n",
    "            # Concatenate heads: [T, num_heads * head_dim]\n",
    "            attn_output = attn_output.transpose(0, 1).reshape(T, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Final projection: [T, emb_dim]\n",
    "        if project: \n",
    "            attn_output = self.out_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "# Example usage:\n",
    "torch.manual_seed(42)\n",
    "token_encoding = torch.tensor([[1.16, 0.23],\n",
    "                               [0.57, 1.36],\n",
    "                               [4.41, -2.16]])\n",
    "# Single-head attention\n",
    "mha_single = MyMultiHeadAttention(emb_dim=2, num_heads=1)\n",
    "out_single, attn_weights = mha_single(token_encoding)\n",
    "print(f\"Single-head output:\\n{out_single}\\nWeights:\\n{attn_weights}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Two-head attention ‚Äì if you initialize head0 to match the single-head parameters,\n",
    "# then after the final projection the corresponding contribution from head0 can match.\n",
    "mha_multi = MyMultiHeadAttention(emb_dim=2, num_heads=2)\n",
    "out_multi, attn_weights = mha_multi(token_encoding, concatenate_heads=False)\n",
    "print(f\"\\nMulti-head (2 heads) output:\\n{out_multi}\\nWeights:\\n{attn_weights}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "out_multi, attn_weights = mha_multi(token_encoding, concatenate_heads=True)\n",
    "print(f\"\\nMulti-head (2 heads) output:\\n{out_multi}\\nWeights:\\n{attn_weights}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "out_multi, attn_weights = mha_multi(token_encoding, concatenate_heads=True, project=True)\n",
    "print(f\"\\nMulti-head (2 heads) output:\\n{out_multi}\\nWeights:\\n{attn_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de1426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
